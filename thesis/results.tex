\chapter{Results and evaluation}
\label{ch:results}

\section{Method}

The learning ghost controller described in chapter \ref{ch:design} was successfully implemented in Java using the IntelliJ development environment, and a server to facilitate testing the project, detailed in chapter \ref{ch:verification}, was developed.  The server makes it possible to specify a set of parameters for the algorithm in a JavaScript file, and allows any number of connected machines to run the games specified by the scripts and report back the final scores.

A number of these parameter scripts were created to investigate various aspects of the algorithm, and these will be detailed below.  Due to the high variance, 20 games were run for each script and an average was obtained for each set.  Reference will be made to whether results are ``statistically significant'': a result is taken to be significantly different from another if a Student's t-test on the two data sets gives $p < 0.05$.

The games were run over 12 identical computers having [such and such a processor] and [an amount] of memory.  Since the computers have four cores, each computer ran two instances of the experiment client: this meant each computer could run two games at once, halving the amount of time needed whilst saving resources and electricity.  The cluster ran up to 24 games simultaneously, and a total of 1900 games were run over a few days; this would have taken 1--2 weeks without the experiment server.

\section{Results}

The first experiment aimed to assess broadly whether the learning ghost controller improved performance.  It was initially assumed that the agent would have poor performance if the learning controller started off with zero knowledge, so it was supplied with weights trained from data recorded in a game against the {\tt Legacy} controller.  Figure \ref{fig:results1} shows the average final scores obtained after playing 20 games using each of the six sample ghost controllers as opponents, first using {\tt Legacy} during playouts as a ``non-learning'' controller, and again using the learning controller.

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={overall,pansy},
    ylabel=Score,
    ybar=5pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.5,
    legend columns=4
    ]
    \addplot coordinates {
       (overall,35601)
       (pansy,29159)
    };
    \addlegendentry{non-learning}
    
    \addplot coordinates {
       (overall,29053)
       (pansy,35885)
    };
    \addlegendentry{learning}
\end{axis}
\end{tikzpicture}
\caption[Learning vs non-learning ({\tt Legacy}) ghost controller]{Learning vs non-learning ({\tt Legacy}) ghost controller: averaged over all sample opponents, there is a decrease in performance when using the learning controller, but taking the {\tt PansyGhosts} controller on its own reveals an increase in performance.  Both results are statistically significant ($p < 0.05$).}
\label{fig:results1}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={non-learning,learning},
    ylabel=Score,
    ybar=5pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.5
    ]
    \addplot coordinates {
       (non-learning,25965)
       (learning,29053)
    };
\end{axis}
\end{tikzpicture}
\caption[Learning vs non-learning ({\tt RandomGhosts}) ghost controller]{Learning vs non-learning ({\tt RandomGhosts}) ghost controller: using the learning controller during rollouts shows slightly improved performance over using a random player, but not significantly so.}
\label{fig:resultsrandom}
\end{figure}

The results look at first glance disappointing, with the average over all 6 opponents being significantly lower when using the learning controller during playouts compared to using {\tt Legacy}; however, further investigation of the data revealed that performance against the {\tt PansyGhosts} controller showed a significant increase in performance as a result of using the learning controller.  Recall from section \ref{sec:sampleghosts} that the {\tt PansyGhosts} controller causes all of the ghosts to run away from Ms~Pac-Man always, whilst the {\tt Legacy} controller causes three of the ghosts to run towards Ms~Pac-Man always: it would seem sensible that {\tt Legacy} would be a bad model for {\tt PansyGhosts} and thus the ability to learn the opposite behaviour of the opponent has clearly benefited the agent.  Figure \ref{fig:resultsrandom} shows that using the learning controller might be slightly better than using a random player, but the result is not statistically significant.

It was conjectured that the reason for the lowered performance overall when using the learning controller is that time spent learning takes away from the time available to the MCTS algorithm, potentially leading to lower quality decisions.  Therefore the first experiment was repeated with the length of playouts (that is, the amount of \emph{game time} each playout is simulated for, not the length of time the simulation is run for) capped at 10 seconds, in the hope that the algorithm would be able to perform more playouts.  Figure \ref{fig:resultssr} shows that the learning controller still gives a decreased performance; in retrospect, the 10 second cap may still have been too high to really effect the outcome.

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={non-learning,learning},
    ylabel=Score,
    ybar=5pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.5
    ]
    \addplot coordinates {
       (non-learning,35721)
       (learning,30042)
    };
\end{axis}
\end{tikzpicture}
\caption[Learning with a 10 second maximum rollout length]{Learning with a 10 second maximum rollout length: capping the rollout length at 10 seconds does not seem to alter the results any.}
\label{fig:resultssr}
\end{figure}

In order to properly exclude the possibility that the time spent learning decreases the quality of the MCTS results, the first experiment was repeated; instead of running in real-time however, every decision was given exactly 1000 iterations of MCTS and the ghost neural networks were trained for exactly 100 iterations on every ghost decision observed.  The results in figure \ref{fig:resultsrealtime} still show a decrease in overall performance when using the learning controller during playouts, but it is not a statistically significant difference.  The {\tt PansyGhosts} controller still shows a significant increase in performance.  It is not clear why the learning controller should not be at least as good as using {\tt Legacy} during playouts when the number of iterations of MCTS is fixed.

The experiments described so far have used the learning controller initiated with weights trained from data recorded in a game against {\tt Legacy} (i.e., the controller should start off behaving like {\tt Legacy}).  This is because it was initially assumed that the agent would do really poorly to start off with until the ghost controller learned some behaviour, and that it may never get the chance to learn behaviour if the agent died early on as a result of poor decisions.  It was decided to verify this belief, and so 20 games were run against all 6 opponent controllers for each of the following four cases: real-time with and without pre-trained weights, and non-real-time with and without pre-trained weights.  The results are shown in figure \ref{fig:resultsuntrained}, and they are somewhat surprising: using pre-trained weights confers no advantage.  It is conjectured that the ghost behaviour is actually rather easy to learn, and that it does not take long for this to happen.

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={overall,pansy},
    ylabel=Score,
    ybar=5pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.5,
    legend columns=4
    ]
    \addplot coordinates {
       (overall,38646)
       (pansy,35956)
    };
    \addlegendentry{non-learning}
    
    \addplot coordinates {
       (overall,34767)
       (pansy,44950)
    };
    \addlegendentry{learning}
\end{axis}
\end{tikzpicture}
\caption[Non-realtime learning]{Non-realtime learning: when every decision is given exactly 1000 iterations of MCTS and the ghost networks are trained exactly 100 times for every new example, the learning controller still fares slightly worse overall, but the result is not statistically significant.  On the other hand, {\tt PansyGhosts} taken on its own shows a statistically significant improvement with learning.}
\label{fig:resultsrealtime}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={realtime,non-realtime},
    ylabel=Score,
    ybar=5pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.5,
    legend columns=4
    ]
    \addplot coordinates {
       (realtime,28716)
       (non-realtime,34974)
    };
    \addlegendentry{untrained}
    
    \addplot coordinates {
       (realtime,29053)
       (non-realtime,34767)
    };
    \addlegendentry{pre-trained}
\end{axis}
\end{tikzpicture}
\caption[Untrained vs pretrained]{Untrained vs pretrained: supplying the learning controller with weights trained from {\tt Legacy} to start from confers no significant advantage, either in realtime or not.}
\label{fig:resultsuntrained}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
\pgfplotsset{every axis legend/.append style={
at={(0.5,-0.2)},
anchor=north}}
\begin{axis}[
    symbolic x coords={20,50,100,200,500},
    ylabel=Score,
    ybar=35pt,
    xtick=data,
    ymin=0,
    nodes near coords,
    scaled y ticks=false,
    axis lines*=left,
    bar width=30pt,
    enlarge x limits=0.2,
    ]
    \addplot coordinates {
       (20,34081)
       (50,34096)
       (100,34767)
       (200,33182)
       (500,34453)
    };
\end{axis}
\end{tikzpicture}
\caption[The effect of varying the number of training iterations]{The effect of varying the number of training iterations: there is no significant difference between any of the values.}
\label{fig:resultsiterations}
\end{figure}



Finally, a series of experiments were run in non-real-time mode to investigate the effect of changing the number of iterations each new observed training example was learned over.  20 games were played against each of the 6 opponents, for each of the following iteration counts: 20, 50, 100, 200 and 500.  In all cases, the number of iterations of MCTS run was 1000.  Figure \ref{fig:resultsiterations} shows the results: varying the number of iterations of training does not alter the average final score achieved.  This lends further evidence to the conjecture that the ghost behaviour is particularly trivial for the network to learn.

