\chapter{Summary and conclusions}
\label{ch:summary}

\section{Summary}
An existing agent for Ms~Pac-Man utilising Monte Carlo tree search was augmented by adding a ghost controller to use during MCTS playouts that is capable of learning its behaviour from the opponent ghost team by using neural networks.  In order to evaluate the additions, an ``experiment server'' was created to allow experiments to be specified in JavaScript files and farmed out to many computers to be executed concurrently.

A total of 1900 games were run using the server to investigate the effect of changing various parameters in the algorithm.  The experiment server was a definite success, allowing these games to be run over a few days rather than the week or two it would have otherwise taken.  The success of the learning ghost controller is summarised in the following section.

\section{Conclusions}

As shown in chapter \ref{ch:results}, using the learning controller during playouts instead of a non-learning controller such as {\tt Legacy} appears to decrease the performance of the original agent when averaged over all of the sample ghost controllers in the framework.  However, the framework ghost teams are reasonably similar for the most part, and using the {\tt Legacy} controller during playouts gives reasonable results.

On the other hand, the {\tt PansyGhosts} controller is almost exactly opposite to {\tt Legacy} in that it always runs away from Ms~Pac-Man rather than always running towards her, and using the learning controller during playouts against this team shows a significant improvement in the score over using {\tt Legacy} in the playouts.  It is clear therefore that the controller is able to learn the behaviour of the opponent, which gives an advantage when the opponent is drastically different from {\tt Legacy}.

The results also indicate that the ghost behaviour is particularly easy to learn.  This suggests that neural networks may not have been the best choice, and that some other form of simpler learning would have been better.

\section{Further work}

The performance of the original agent falls short of other current agents using plain Monte Carlo tree search: it would be prudent to investigate the reasons for this and attempt to bring the ``vanilla'' agent to the same level of performance as its contemporaries.

Further investigation is required to understand why the performance of the learning agent is not even as good as the original agent when playing against most of the framework ghosts.  The agent should also be evaluated against better ghost teams as the ones used are very basic and quite similar in most cases.

Finally, using neural networks may not have been the best choice for this project and other machine learning techniques could be investigated.  Section \ref{sec:alternativetreatments} gives several suggestions, such as using a classification algorithm or reinforcement learning.
